
$ssh ceph-node2
$ lsblk
#disk space
df

B- Configure Ansible Inventory and Ansible Playbook for ceph


a- The first node (mon1) will be used as the administration node for the installation.

[cephuser@mon1 ceph-ansible]$  cp group_vars/all.yml.sample group_vars/all.yml
[cephuser@mon1 ceph-ansible]$

b-Configuring deployment files


$nano group_vars/all.yml
#General
cluster: ceph
# Inventory host group variables
mon_group_name: mons
osd_group_name: osds
rgw_group_name: rgws
mds_group_name: mdss
mgr_group_name: mgrs
#rgwloadbalancer_group_name: rgwloadbalancers
grafana_server_group_name: grafana-server
# Ceph packages
ceph_origin: repository
ceph_repository: community
ceph_repository_type: cdn
ceph_stable_release: pacific
# Interface options
monitor_interface: enp1s0
radosgw_interface: enp1s0
public_network: 192.168.121.0/24
cluster_network: 192.168.121.0/24
#DASHBOARD
dashboard_enable: True
dashboard_protocole: https
dashboard_port: 8443
dashboard_admin_user: admin
dashboard_admin_password: Kxsandra202
#Graphana
grafana_admin_user: admin
grafana_admin_password: Kxsandra202
grafana_datasource: Dashboard
grafana_dashboard_version: pacific
grafana_port: 3000
grafana_allow_embedding: True


b-Create a new ansible inventory of ceph nodes. Set your inventory file correctly. Below is an inventory generated by creating the hosts file in the current directory. 
Modify the inventory groups to how you want the services installed in your cluster nodes.

$ nano ansible.cfg

# Be sure the user running Ansible has permissions on the logfile
log_path = $HOME/vagrant/ansible.log

inventory=/home/vagrant/ceph-ansible/hosts


$ nano hosts
ceph-node1 ansible_user=vagrant ansible_become=true ansible_become_method=sudo ansible_become_user=root ansible_sudo_pass=vagrant
ceph-node2 ansible_user=vagrant ansible_become=true ansible_become_method=sudo ansible_become_user=root ansible_sudo_pass=vagrant

[monprimary]
ceph-node1
# Noeud moniteur
[mons]
ceph-node1
# MDS Nodes
[mdss]
ceph-node1
## RGW
#[rgws]
#ceph-node1
#Noeuds du deamon Manager
[mgrs]
ceph-node1
# (Object Storage Daemon) Nodes
[osds]
ceph-node1
# Serveur Grafana
[grafana-server]
ceph-node1


additionnal files to configure


[vagrant@mon1 ceph-ansible]$ cp site.yml.sample site.yml
[vagrant@mon1 ceph-ansible]$


[vagrant@mon1 ceph-ansible]$  cp group_vars/osds.yml.sample group_vars/osds.yml
[vagrant@mon1 ceph-ansible]$
 
$
$ nano group_vars/osds.yml
#---------------Option 1---------
osd_auto_discovery: true

C- Cluster deployment:

$ ansible-playbook -u vagrant site.yml -i hosts

#Once playbook completes the Ceph cluster installation job and plays the recap with failed=0, it means ceph-ansible has deployed the Ceph cluster

