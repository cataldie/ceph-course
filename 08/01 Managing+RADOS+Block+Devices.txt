# Managing RADOS Block Devices

The goal of this guide is to demonstrate the creation and management of RADOS block device images, which can be utilized as regular block devices. Before proceeding with the steps, ensure that the Ceph cluster status is HEALTH_OK by executing the following command:

```bash
ceph health
```

## Step 1: Create Replicated Pool with Application Type

1. Create a replicated pool called "labpool1" with 32 placement groups using the following command:

```bash
ceph osd pool create labpool1 32 32
```

2. Set the application type for the pool to "rbd" using:

```bash
rbd pool init labpool1
```
check the usaget store
$ ceph df

3. Generate the necessary authentication credentials for the pool:

```bash
ceph auth get-or-create client.labpool1.client1 \
    mon 'profile rbd' osd 'profile rbd' \
    -o /etc/ceph/ceph.client.labpool1.client1.keyring
```

$ more /etc/ceph/ceph.client.labpool1.client1.keyring

$ ceph auth get client.labpool1.client1

## Step 2: Configure the Client Node

1. On the client node (clientb), install the required Ceph packages:

1. Install the required packages:
```
sudo apt update -y
sudo apt install ceph-common -y
```

root@ceph-client1:~# cd /etc/ceph/
root@ceph-client1:/etc/ceph# ls
rbdmap

root@ceph-node1:/# scp /etc/ceph/ceph.conf root@ceph-client1:/etc/ceph
root@ceph-node1:/# scp /etc/ceph/ceph.client.labpool1.client1.keyring root@ceph-client1:/etc/ceph


2. Export CEPH_ARGS to specify the client's ID:
try ceph df not permitted, then
```bash
root@ceph-client1:~# export CEPH_ARGS='--id=labpool1.client1'
```

3. Verify the cluster connection:

```bash
rbd ls labpool1
```
if no error is ok

## Step 3: Create and Mount RBD Images

1. Create an RBD image in "labpool1" named "test" with a size of 128 MB:

```bash
rbd create labpool1/test --size=128M
```

```bash
rbd ls labpool1
```
check the information of this image
$ rbd info labpool1/test
Now, this image has not yet mapped to my cluster.
$ rbd showmapped

2. Map the RBD image on the clientb node using the kernel RBD client:

```bash
rbd map labpool1/test
```

3. Show mapped RBD devices:

```bash
rbd showmapped
```

4. Format the mapped RBD image with XFS:

```bash
mkfs.xfs /dev/rbd0
```
there is not the device
df -h 

5. Create a mount point:

```bash
mkdir /mnt/rbd
```

6. Mount the RBD image to the mount point:

```bash
mount /dev/rbd0 /mnt/rbd
```

now there is the device
df -h 

7. Review the file system usage:

```bash
df /mnt/rbd
```

8. Add content to the file system (e.g., create a file named "test1"):

```bash
dd if=/dev/zero of=/mnt/rbd/test1 bs=10M count=1
```
9. Verify the content and file size:

```bash
cd /mnt/rbd
ls 
ll -h
df
```
$ cd
check the client pool from the node note that we crated 10mb  of the data but the cluser see 35mb
Just because this folder, this pulsar is a replicated pool and the default number of replica is three.
root@ceph-node1:/# ceph df


## Step 4: Unmap and Configure Persistent Mounting

1. Unmount the file system and unmap the RBD image:

```bash
root@ceph-client1:~# umount /mnt/rbd
rbd unmap /dev/rbd0
```

root@ceph-client1:~# more /etc/ceph/rbdmap 
# RbdDevice		Parameters
#poolname/imagename	id=client,keyring=/etc/ceph/ceph.client.keyring


2. Configure the client system to persistently mount the RBD image as "/mnt/rbd."

3. Create an entry for "labpool1/test" in the RBD map file:

```bash
echo "labpool1/test id=labpool1.client1,keyring=/etc/ceph/ceph.client.labpool1.client1.keyring" >> /etc/ceph/rbdmap
```

4. Create an entry for "/dev/rbd/labpool1/test" in the "/etc/fstab" file:

```bash
echo "/dev/rbd/labpool1/test /mnt/rbd xfs noauto 0 0" >> /etc/fstab
```

5. Re-map the RBD device:

```bash
rbdmap map
```

6. Check the mapped devices:

```bash
rbd showmapped
```

rbdmap unmap
rbd showmapped

7. Enable the rbdmap service to ensure persistent mounting:

```bash
systemctl enable rbdmap
```

8. Reboot the clientb node to verify that the RBD device mounts persistently.
reboot now
df -h # The device has been automatically mounted by the service.

root@ceph-client1:~# df | grep rbd
/dev/rbd0                            123584   18180    105404  15% /mnt/rbd
root@ceph-client1:~# rbdmap  unmap
root@ceph-client1:~# df | grep rbd
df -h 

## Step 5: Clean Up
comment o remove:
"/dev/rbd/labpool1/test /mnt/rbd xfs noauto 0 0" >> /etc/fstab
"labpool1/test id=labpool1.client1,keyring=/etc/ceph/ceph.client.labpool1.client1.keyring" >> /etc/ceph/rbdmap

1. If needed, remove the RBD image:

```bash
rbd rm labpool1/test --id labpool1.client1
```

2. List the contents of the "labpool1" pool:

```bash
rados -p labpool1 ls --id labpool1.client1
```
root@ceph-node1:/# ceph df
# and check that the labpool1 has no used space

Please ensure to follow these instructions carefully to create and manage RADOS block devices effectively.